# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aqhgS46THD0jBZWg5YTQtqnhmGq_ih-
"""

#install library kaggle untuk download dataset
!pip install kaggle

#buat directory .kaggle
!mkdir .kaggle
!ls -a

#buat token untuk bisa akses download dataset
import json, os
token = {"username":"donodanar35","key":"92c113ad1a2fdabc92ea0f3fe5666e90"}

with open('/content/.kaggle/kaggle.json', 'w') as file:
    json.dump(token, file)

!chmod 600 /content/.kaggle/kaggle.json
!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json

os.listdir('.kaggle/')
!kaggle config set -n path -v{/content}

#download dataset IMDB - dataset yang berisi review film dengan 50K data dan 2 sentimen (positif dan negatif)
!kaggle datasets download -d columbine/imdb-dataset-sentiment-analysis-in-csv-format -p /tmp

import zipfile,os
local_zip = '/tmp/imdb-dataset-sentiment-analysis-in-csv-format.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')

#ekstra dataset file zip ke destinasi /tmp
zip_ref.extractall('/tmp')
zip_ref.close()

os.listdir('/tmp')

#baca dataset IMB menggunakan dataframe pandas
import pandas as pd

df = pd.read_csv('/tmp/Train.csv', names=['text', 'label'], sep=',')
df = df.drop(0)
df

df.info()

from sklearn.model_selection import train_test_split

#pisahkan dataset training dan dataset validation sebenar 80% dan 20%
review = df['text'].values
y = df['label'].values
review_latih, review_val, y_latih, y_val = train_test_split(review, y, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
     
#lakukan tokenisasi
tokenizer = Tokenizer(num_words=500, oov_token='-')
tokenizer.fit_on_texts(review_latih) 
tokenizer.fit_on_texts(review_val)

#lakukan sequence     
sekuens_latih = tokenizer.texts_to_sequences(review_latih)
sekuens_val = tokenizer.texts_to_sequences(review_val)
     
padded_latih = pad_sequences(sekuens_latih, padding='post', maxlen=300,truncating='post') 
padded_val = pad_sequences(sekuens_val, padding='post', maxlen=300,truncating='post')

#convert dataset label (y) menjadi float biar bisa dibaca tensorflow
y_latih = np.asarray(y_latih).astype('float').reshape((-1,1))
y_val = np.asarray(y_val).astype('float').reshape((-1,1))

y_latih[100]

padded_latih[100]

import tensorflow as tf

#bangun model
n = len(tokenizer.word_index)
model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=n, output_dim=16),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid') #gunakan activation sigmoid untuk 2 kelas data 
])

model.compile(loss='binary_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

#buat visualisasi model
model_visual = tf.keras.utils.plot_model(model,
                          to_file='model.png',
                          show_shapes=True,
                          show_layer_names=True,
                          rankdir='TB',
                          expand_nested=True,
                          dpi=55)
model_visual

#buat ringkasan model
model.summary()

#buat kelas dan fungsi callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.85):
      if(logs.get('accuracy')>0.85):      
        print("\nAkurasi data latih dan validation telah mencapai >85%!")
        self.model.stop_training = True
callbacks = myCallback()

#lakukan pelatihan model
history = model.fit(padded_latih, y_latih, epochs=80, 
                    validation_data=(padded_val, y_val), validation_steps=20, verbose=2, callbacks=[callbacks])

import matplotlib.pyplot as plt

history_dict = history.history

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss=history_dict['loss']
val_loss=history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12,9))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(12,9))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim((0.5,1))
plt.show()