# -*- coding: utf-8 -*-
"""Animal_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ra5gVTbeB8A7QKIcqj9fpRKGIhzZKcVw
"""

#install library kaggle untuk download dataset
!pip install kaggle

#buat directory .kaggle
!mkdir .kaggle
!ls -a

#buat token untuk bisa akses download dataset
import json, os
token = {"username":"donodanar35","key":"92c113ad1a2fdabc92ea0f3fe5666e90"}

with open('/content/.kaggle/kaggle.json', 'w') as file:
    json.dump(token, file)

!chmod 600 /content/.kaggle/kaggle.json
!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json

os.listdir('.kaggle/')
!kaggle config set -n path -v{/content}

#download food11-image-dataset.zip dari kaggle untuk klasifikasi keadaan tanaman tomat dengan 10 kelas
!kaggle datasets download -d andrewmvd/animal-faces -p /tmp

os.listdir('/tmp')

import zipfile,os
local_zip = '/tmp/animal-faces.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')

#ekstra dataset file zip ke destinasi /tmp
zip_ref.extractall('/tmp')
zip_ref.close()

os.listdir('/tmp')

os.listdir('/tmp/afhq/train')

base_dir = '/tmp/afhq'

#lihat base_dir dan buat folder dataset_dir
os.listdir(base_dir)
dataset_dir = os.path.join(base_dir, 'train')

file_name = []
tag = []
full_path = []
for path, subdirs, files in os.walk(dataset_dir):
    for name in files:
        full_path.append(os.path.join(path, name)) 
        tag.append(path.split('/')[-1])        
        file_name.append(name)
                         
import pandas as pd
df = pd.DataFrame({"path":full_path,'file_name':file_name,"tag":tag})
df.groupby(['tag']).size()

df.tail(5)

df.info()

from sklearn.model_selection import train_test_split
X= df['path']
y= df['tag']

#bagi dataset menjadi 2 yakni dataset validation dan dataset training
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=0)

df_tr = pd.DataFrame({'path':X_train
              ,'tag':y_train
             ,'set':'train'})

df_test = pd.DataFrame({'path':X_test
              ,'tag':y_test
             ,'set':'test'})

#cetak banyak data train dan data testing
print('train size', len(df_tr))
print('test size', len(df_test))

# melihat proporsi pada masing masing set apakah sudah ok atau masih ada yang ingin diubah
df_all = df_tr.append([df_tr,df_test]).reset_index(drop=1)\

#cetak dataframe df_all gabungan dari df_tr, df_val
print(df_all.groupby(['set','tag']).size(),'\n')

#mengecek sample data
df_all.sample(3)

import shutil
from tqdm.notebook import tqdm as tq

datasource_path = "tmp/afhq/train"
dataset_path = "dataset/pelatihan"

for index, row in tq(df_tr.iterrows()):
    
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['tag'],row['image'].split('.')[0])            
    
    if os.path.exists(os.path.join(dataset_path,row['set'],row['tag'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['tag']))
    
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['tag'],destination_file_name)
    
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

datasource_path = "tmp/afhq/train"
dataset_path = "dataset/ujicoba"

for index, row in tq(df_test.iterrows()):
    
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['tag'],row['image'].split('.')[0])            
    
    if os.path.exists(os.path.join(dataset_path,row['set'],row['tag'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['tag']))
    
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['tag'],destination_file_name)
    
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

os.listdir('dataset/pelatihan/train')

import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#buat image data generator untuk dataset training 
train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

#buat image data generator untuk dataset testing
test_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

train_generator = train_datagen.flow_from_directory(
        'dataset/pelatihan/train/',  
        target_size=(150, 150), 
        batch_size=4,
        class_mode='categorical') #gunakan categorical untuk banyak kelas data pada dataset training

validation_generator = test_datagen.flow_from_directory(
        'dataset/ujicoba/test/', 
        target_size=(150, 150), 
        batch_size=4, 
        class_mode='categorical') #gunakan categorical untuk banyak kelas data pada dataset testing

#buat model sequential
model = tf.keras.models.Sequential([
    #buat masing-masing layer konvolusi, layer maxpooling                                    
    tf.keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    #buat hidden layer dense
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    #buat inputan flatten
    tf.keras.layers.Flatten(),
    #buat output dense dengan aktivasi softmax
    tf.keras.layers.Dense(3, activation='softmax')
])

#compile model
model.compile(loss='categorical_crossentropy', 
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

#buat visualisasi model
model_visual = tf.keras.utils.plot_model(model,
                          to_file='model.png',
                          show_shapes=True,
                          show_layer_names=True,
                          rankdir='TB',
                          expand_nested=True,
                          dpi=55)
model_visual

model.summary()

#buat kelas dan fungsi callback
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.92):
      if(logs.get('accuracy')>0.92):      
        print("\nAkurasi data latih dan testing telah mencapai >92%!")
        self.model.stop_training = True
callbacks = myCallback()

#lakukan training data
history = model.fit(
   train_generator,
   steps_per_epoch = 5, 
   epochs = 500,
   validation_data = validation_generator, 
   validation_steps = 5,  
   verbose = 2,
   callbacks=[callbacks])

import matplotlib.pyplot as plt

history_dict = history.history

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss=history_dict['loss']
val_loss=history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12,9))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(12,9))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim((0.5,1))
plt.show()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)